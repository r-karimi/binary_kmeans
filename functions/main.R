# This file contains the main functions used in the Binary K-means repository.

## cluster_paste
# Arguments: 
  # recluster_set: A list of clusters marked for reclustering.
  # reclust_data: A list of kmeans-type object applied to each of the clusters marked for reclustering.
# Returns:
  # A list of reclustered clusters with new cluster indices.
# Description:
  # This function deals with indexing issues associated with reclustering,
  # ensuring that each cluster in the returned object has a unique cluster
  # number in the correct order corresponding to minimum times.

cluster_paste = function(recluster_set, reclust_data){
  #extract the data columns
  for(i in 1:length(recluster_set)){
    if(i == 1){
      recluster_set[[i]] = tibble(time = recluster_set[[i]][[1]], 
                                  intensity = recluster_set[[i]][[2]], 
                                  .cluster = as.numeric(reclust_data[[i]][["cluster"]]))
    } else { # this is a successful implementation of index bumping!
      recluster_set[[i]] = tibble(time = recluster_set[[i]][[1]],
                                  intensity = recluster_set[[i]][[2]],
                                  .cluster = reclust_data[[i]][["cluster"]] + 
                                    max(recluster_set[[i-1]][[".cluster"]]))
    }

    colnames(recluster_set[[i]]) = c("time", "intensity", ".cluster")
    
    recluster_set[[i]] = recluster_set[[i]] %>% as_tibble()
  }
  
  return(recluster_set)
}

## re_clust
# Arguments: 
  # clusters: A list of clusters as generated by a round of
  # binary segmentation.
# Returns:
  # A new list of clusters after having undergone binary segmentation of the
  # clusters marked for reclustering.
# Description:
  # This function is one of the main functions of this repository, using
  # several helper functions to carry out the binary segmentation that is
  # iterated in the pipe function.

re_clust = function(clusters){
  # Filter off all the clusters not needing reclustering.
  no_recluster = clusters %>% 
    filter(!reclust)
  
  # Filter off all the clusters marked for reclustering.
  recluster_set = clusters %>% 
    filter(reclust) %>% 
    select(data, .cluster) %>%
    unnest(cols = c(data)) %>%
    group_by(.cluster) %>%
    group_split() 
  
  # Apply the k-means clustering algorithm to each cluster marked
  # for reclustering.
  reclust_data = recluster_set %>%
    map(kmeans_helper)
  
  # Adjust the indices of the resultant reclustered clusters.
  recluster_set = cluster_paste(recluster_set, reclust_data)
  
  # Apply the Shapiro-Wilk test to each cluster 
  reclustered = recluster_set %>% 
    bind_rows() %>%
    group_by(.cluster) %>%
    nest() %>%
    mutate(shap = map(data, shapiro_helper)) %>%
    mutate(reclust = ifelse(shap <= 0.05, T, F)) 
  
  # Adjust the indices of those clusters that didn't need to be reclustered.
  no_recluster$.cluster = no_recluster$.cluster + max(reclustered$.cluster)
  
  # Bind the two sets of clusters and return.
  bind_rows(reclustered, no_recluster) %>% return()
}

## cluster_sort
# Arguments: 
  # clusters: A list of clusters as generated by a round of
  # binary segmentation.
# Returns:
  # A new list of clusters, with cluster numbers assigned in order of minimum time.
# Description:
  # This function sorts the clusters argument in the order of ascending time
  # (i.e. the cluster that contains the tuple with time 0 is assigned the 
  # cluster number 1, the next one 2, and so on). This is important to avoid
  # clusters with non-unique cluster numbers.

cluster_sort = function(clusters){
  # Arrange clusters in order of increasing time.
  sorted_clusters = clusters %>% 
    mutate(min_time = map(data, min_time_helper)) %>%
    unnest(c(data, shap, med, min_time)) %>%
    arrange(min_time) %>%
    nest(data = c(time, intensity))
  
  # Reassign the cluster numbers.
  for(i in 1:length(sorted_clusters$.cluster)){
    sorted_clusters$.cluster[[i]] = i
  }
  
  # Return re-indexed clusters.
  return(sorted_clusters)
}

## cluster_compress
# Arguments: 
  # clusters: A list of clusters as generated by a round of
  # binary segmentation.
  # compression: A compression factor best thought of as a 
  # decimal percentage (i.e. for 15%, input 0.15) 
# Returns:
  # A new list of clusters with clusters compressed (merged) together
  # if their median fluorescence intensity values fall within a certain
  # percentage of the maximum fluorescence intensity of the trace.
# Description:
  # This function uses a user-defined parameter, the compression parameter,
  # to merge clusters with median fluoescence intensity values within a
  # certain distance of the maximum fluorescence intensity of the trace. This
  # was found to better the performance of the algorithm through testing.

cluster_compress = function(clusters, compression){
  # Compute the median value of each cluster.
  clusters = clusters %>% 
    mutate(med = map(data, median_helper)) %>%
    cluster_sort()
  
  # Compute the max intensity of the trace.
  max = clusters %>% 
    unnest(cols = c(data))
  max = max(max$intensity)
  
  # Loop through all the clusters and merge the clusters whose medians
  # fall within a certain threshold.
  index = 1
  while(index <= (length(clusters$med)-1)){
    if(abs(clusters$med[index] - clusters$med[index+1]) <= compression*max){
      clusters[index,] = merge_helper(clusters$data[[index]],
                                      clusters$data[[index+1]], 
                                      clusters$.cluster[index])
      clusters = clusters[-(index+1),]
    } else {
      index = index + 1
    }
  }
  
  # Return the clusters with merges complete.
  return(clusters)
}

## breakpoints
# Arguments: 
  # clusters: A list of clusters as generated by a round of
  # binary segmentation.
# Returns:
  # A new list of clusters with intra-cluster thresholding having
  # been performed on each cluster.
# Description:
  # This function attempts to detect transient level sets in each cluster
  # by examining the lagged differences between points in the first and last
  # 20% of the cluster.

breakpoints = function(clusters){
  # Find the largest cluster number.
  max_cluster = max(clusters$.cluster)
  
  # Apply the breakpoint_helper function to each of the clusters.
  resolved_clusters = clusters %>% 
    unnest(cols = c(data)) %>% 
    group_by(.cluster) %>% 
    group_split() %>% 
    map(breakpoint_helper, max_cluster) %>% 
    bind_rows()
  
  # Adjust indices.
  newIndices = seq(1, nrow(resolved_clusters))
  resolved_clusters$.cluster = newIndices
  
  # Return.
  return(resolved_clusters)
}

## pipe
# Arguments:
  # data: A data frame of raw time-intensity tuples.
  # compression: The compression parameter to be used in cluster_compress.
# Returns:
  # The fully processed clusters detected in the raw time-intensity trace.
# Description:
  # The pipe function applies the full data processing pipeline developed
  # in the Binary K-means repository, in the correct order.

pipe = function(data, compression){
  
  # Eliminate any extraneous columns, and name the columns.
  data = as_tibble(data[,1:2])
  colnames(data) = c("time", "intensity")
  
  # Apply pre-processing functions.
  processed = data %>% 
    find_window() %>% 
    pre_process()
  scale_factor = processed$scale
  
  # Convert data frame into the cluster list format.
  clusters = processed$data %>%
    pre_clust()
  
  # The heart of the algorithm! Perform binary segmentation 10 times 
  # or until all clusters pass the Shapiro-Wilk test 
  # for normality--whichever occurs first (virtually always the latter
  # due to limits in expected subunit stoichiometry of nanotubes).
  it_count = 0
  while((it_count <= 10) && (clusters$reclust %>% sum() != 0)){
    clusters = clusters %>% re_clust()
    it_count = it_count + 1
  }
  
  clusters = clusters %>%
    mutate(med = map(data, median_helper)) %>%
    cluster_sort() %>% # Sort clusters by time.
    breakpoints() %>% # Resolve intra-cluster steps.
    cluster_sort() %>% # Sort again.
    cluster_compress(compression) %>% # Merge clusters with close median values.
    cluster_sort() %>% # Sort one last time.
    unnest(cols = c(data)) %>% # Transform the data back to the original length scales.
    mutate(intensity = intensity/scale_factor) %>%
    group_by(.cluster) %>%
    nest()
  
  # Return the processed clusters!
  return(clusters)
}